{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# LOAD SST2 DATASET\n",
    "DATA_SST_TRAIN = pd.read_csv('./assets/datasets/SST2/train.tsv', sep='\\t')\n",
    "DATA_SST_TEST = pd.read_csv('./assets/datasets/SST2/test.tsv', sep='\\t')\n",
    "DATA_SST_DEV = pd.read_csv('./assets/datasets/SST2/dev.tsv', sep='\\t')\n",
    "\n",
    "# LOAD QUORA QUESTION PAIRS\n",
    "columns = ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
    "types_dict = {'id': int, 'qid1': int, 'qid2': int , 'question1': str, 'question2': str, 'is_duplicate': int}\n",
    "DATA_QQP_ALL = pd.read_csv('./assets/datasets/QQP/QQP_dataset_all.tsv', sep='\\t', error_bad_lines=True, dtype=types_dict)\n",
    "DATA_QQP_ALL['is_duplicate'] = DATA_QQP_ALL['is_duplicate'].fillna(value=0)\n",
    "DATA_QQP_ALL = DATA_QQP_ALL.fillna(value='sentence missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364031 39122 1137\n"
     ]
    }
   ],
   "source": [
    "random_signal = np.random.rand(len(DATA_QQP_ALL))\n",
    "mask_train = random_signal < 0.9\n",
    "mask_test = (random_signal > 0.9) & (random_signal < 0.997)\n",
    "mask_dev = random_signal > 0.997\n",
    "\n",
    "train = DATA_QQP_ALL[mask_train]\n",
    "test = DATA_QQP_ALL[mask_test]\n",
    "dev = DATA_QQP_ALL[mask_dev]\n",
    "print(len(train), len(test), len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('./assets/datasets/QQP/train.tsv', sep='\\t')\n",
    "#test.to_csv('./assets/datasets/QQP/test.tsv', sep='\\t')\n",
    "#dev.to_csv('./assets/datasets/QQP/dev.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
    "types_dict = {'id': int, 'qid1': int, 'qid2': int , \n",
    "              'question1': str, 'question2': str, 'is_duplicate': int}\n",
    "DATA_QQP_TRAIN = pd.read_csv('./assets/datasets/QQP/train.tsv', sep='\\t', dtype=types_dict)\n",
    "DATA_QQP_TEST = pd.read_csv('./assets/datasets/QQP/test.tsv', sep='\\t', dtype=types_dict)\n",
    "DATA_QQP_DEV = pd.read_csv('./assets/datasets/QQP/dev.tsv', sep='\\t', dtype=types_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           0   0     1     2   \n",
       "1           1   1     3     4   \n",
       "2           2   2     5     6   \n",
       "3           3   3     7     8   \n",
       "4           4   4     9    10   \n",
       "5           5   5    11    12   \n",
       "6           6   6    13    14   \n",
       "7           7   7    15    16   \n",
       "8           8   8    17    18   \n",
       "9           9   9    19    20   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_QQP_TRAIN.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Custom imports\n",
    "from model.utils import *\n",
    "from model.data_utils import *\n",
    "from model.transformer import Transformer\n",
    "from model.bracketing import IdentityChunker, NNSimilarityChunker, cos\n",
    "from model.generators import IdentityGenerator, EmbeddingGenerator\n",
    "from model.classifiers import AttentionClassifier, SeqPairAttentionClassifier, NaivePoolingClassifier, SeqPairFancyClassifier\n",
    "from model.model import MultiTaskNet, End2EndModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "\n",
    "########### LOAD MODELS AND OPTIMIZER ###########\n",
    "transformer_net = Transformer(model_class=BertModel,\n",
    "                              tokenizer_class=BertTokenizer,\n",
    "                              pre_trained_weights='bert-base-uncased',\n",
    "                              device=device)\n",
    "\n",
    "bracketing_net = NNSimilarityChunker(sim_function=cos,\n",
    "                                     threshold=0.7,\n",
    "                                     exclude_special_tokens=False,\n",
    "                                     combinatorics='sequential',\n",
    "                                     device=device)\n",
    "\n",
    "generator_net = EmbeddingGenerator(pool_function=abs_max_pooling, \n",
    "                                   device=device)\n",
    "\n",
    "seq_classifier = AttentionClassifier(embedding_dim=768,\n",
    "                                     sentset_size=2,\n",
    "                                     dropout=0.3,\n",
    "                                     n_sentiments=2,\n",
    "                                     pool_mode='concat',\n",
    "                                     device=device).to(device)\n",
    "\n",
    "seq_pair_classifier = SeqPairFancyClassifier(embedding_dim=768,\n",
    "                                             num_classes=2,\n",
    "                                             dropout=0.3,\n",
    "                                             n_attention_vecs=2,\n",
    "                                             device=device)#.to(device)\n",
    "\n",
    "naive_classifier = NaivePoolingClassifier(embedding_dim=768, \n",
    "                                          num_classes=2, \n",
    "                                          dropout=0., \n",
    "                                          pool_mode='max_pooling', \n",
    "                                          device=device).to(device)\n",
    "\n",
    "multitask_net = MultiTaskNet(seq_classifier,\n",
    "                             seq_pair_classifier,\n",
    "                             device=device).to(device)\n",
    "\n",
    "bracketing_net = IdentityChunker().to(device)\n",
    "generator_net = IdentityGenerator().to(device)\n",
    "\n",
    "model = End2EndModel(transformer=transformer_net,\n",
    "                     bracketer=bracketing_net,\n",
    "                     generator=generator_net,\n",
    "                     multitasknet=multitask_net,\n",
    "                     device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gc\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model.data_utils import get_batch_SST2_from_indices, get_batch_QQP_from_indices\n",
    "\n",
    "DATA_QQP_TRAIN = DATA_QQP_ALL.iloc[0:300000]\n",
    "DATA_QQP_DEV = DATA_QQP_ALL.iloc[300000:300100] # 1000 samples to evaluate the performance\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter(log_dir='./tensorboard/', comment='Test run to see how this works!')\n",
    "\n",
    "eval_periodicity = 20\n",
    "\n",
    "counter = {'SST2': 0, 'QQP': 0}\n",
    "batch_size = {'SST2': 24, 'QQP': 24}\n",
    "n_batches = {'SST2': math.floor(len(DATA_SST_TRAIN)/24), 'QQP': math.floor(len(DATA_QQP_TRAIN)/24)}\n",
    "get_batch_function = {'SST2': get_batch_SST2_from_indices, 'QQP': get_batch_QQP_from_indices}\n",
    "dataframe = {'SST2': DATA_SST_TRAIN, 'QQP': DATA_QQP_TRAIN}\n",
    "dev_dataframes_dict = {'SST2': DATA_SST_DEV, 'QQP': DATA_QQP_DEV}\n",
    "datasets = ['SST2','QQP'] # here the datasets in training\n",
    "batch_indices = {}\n",
    "\n",
    "\n",
    "global_counter, losseval, acceval = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_model_on_DF(model, dataframes_dict, batch_size=16, global_counter=0):    \n",
    "    k=0\n",
    "    metrics_dict = {}\n",
    "    for dataset, df in dataframes_dict.items():\n",
    "        n_batches = math.floor(len(df)/batch_size)\n",
    "        batch_splits = [-1]*(len(dataframes_dict)+1)\n",
    "        batch_splits[k] = 0 # [-1, -1, 0, -1, -1]\n",
    "        batch_splits[k+1] = len(df)\n",
    "        k += 1\n",
    "        dev_acc = 0\n",
    "        for i in range(n_batches):\n",
    "            batch_targets, batch_sequences = [], []\n",
    "            indices = list(range(i*batch_size, (i+1)*batch_size))\n",
    "            dataset_batch = get_batch_function[dataset](df, indices)\n",
    "            # construct targets\n",
    "            batch_targets.append(torch.tensor([data[1] for data in dataset_batch], \n",
    "                                              dtype=torch.int64, \n",
    "                                              device=device))\n",
    "            # construct sequences\n",
    "            batch_sequences.extend([data[0] for data in dataset_batch])\n",
    "            batch_predictions = model.forward(batch_sequences, batch_splits=batch_splits)\n",
    "            L = model.loss(batch_predictions, batch_targets, weights=None)\n",
    "            m = model.metrics(batch_predictions, batch_targets)\n",
    "            dev_acc += m[0]\n",
    "            # Log to tensorboard\n",
    "        acc = dev_acc/n_batches\n",
    "        metrics_dict[dataset] = acc\n",
    "        #writer.add_scalars(f'Metrics/{dataset}/dev', {dataset: acc}, global_counter)\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "eval metrics are: {'SST2': 0.5092592592592593, 'QQP': 0.5520833333333334}\n",
      "################### GLOBAL COUNTER 0 ###################\n",
      "Iterations per second: 0.09311179208030342\n",
      "Accuracies QQP: 0.0\n",
      "Global Loss: 0.030136942863464355\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "eval metrics are: {'SST2': 0.6215277777777778, 'QQP': 0.5520833333333334}\n",
      "################### GLOBAL COUNTER 20 ###################\n",
      "Iterations per second: 0.021367616518102133\n",
      "Accuracies QQP: 0.0\n",
      "Global Loss: 0.6701055437326431\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-17c73d272aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mbatch_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/zeta-alpha/semantic-compression/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences_batch, batch_splits)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch_splits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mcontext_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# print('output of transformer size:', context_representation.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbracketer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/zeta-alpha/semantic-compression/model/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_sequences, output_layer, return_masks)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         hidden_states_tup = self.model(batch_input_ids, \n\u001b[0;32m---> 24\u001b[0;31m                                        attention_mask=masks_dict['padding_mask'])[-1]\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_masks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    738\u001b[0m                                        \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                                        \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                                        encoder_attention_mask=encoder_extended_attention_mask)\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mmixed_key_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/za_conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(multitask_net.parameters(), \n",
    "                             lr=0.0001,\n",
    "                             betas=(0.9, 0.999), \n",
    "                             eps=1e-08, \n",
    "                             weight_decay=0.0001, \n",
    "                             amsgrad=False)\n",
    "\n",
    "finished_training = False\n",
    "t = time.time()\n",
    "while not finished_training:\n",
    "    for dataset in datasets:\n",
    "        if counter[dataset] >= n_batches[dataset] or global_counter == 0:\n",
    "            counter[dataset] = 0\n",
    "            # Re-shuffle the training batches data\n",
    "            batch_indices[dataset] = torch.randperm(n_batches[dataset]*batch_size[dataset],\n",
    "                                                    device=torch.device('cpu')).reshape(-1, batch_size[dataset])\n",
    "    \n",
    "    batch_sequences, batch_targets, batch_splits = [], [], [0]\n",
    "    for dataset in datasets:\n",
    "        idx = counter[dataset]\n",
    "        dataset_batch = get_batch_function[dataset](dataframe[dataset], \n",
    "                                                    batch_indices[dataset][idx, :])\n",
    "        # List of tensors, one for each task\n",
    "        try:\n",
    "            batch_targets.append(torch.tensor([data[1] for data in dataset_batch], \n",
    "                                            dtype=torch.int64, \n",
    "                                            device=device))\n",
    "        except:\n",
    "            L = [data[1] for data in dataset_batch]\n",
    "            \n",
    "            raise ValueError(f'This thing failed when the target tensor was in dataset {dataset}: {L}, indices: {batch_indices[dataset][idx, :]}')\n",
    "        \n",
    "        # Big list combining the input sequences/ tuple of sequences because the batch needs\n",
    "        # to be at the same \"depth\" level\n",
    "        batch_sequences.extend([data[0] for data in dataset_batch])\n",
    "        batch_splits.append(batch_splits[-1] + len(dataset_batch))\n",
    "        counter[dataset] += 1\n",
    "\n",
    "    model.train()\n",
    "    batch_predictions = model.forward(batch_sequences, batch_splits=batch_splits)\n",
    "    L = model.loss(batch_predictions, batch_targets, weights=None)\n",
    "    metrics = model.metrics(batch_predictions, batch_targets)\n",
    "    # Log to tensorboard\n",
    "    writer.add_scalar('Loss/train', L.item(), global_counter)\n",
    "    writer.add_scalars('Metrics/train', {datasets[i]: metrics[i] for i in range(len(datasets))}, global_counter)\n",
    "    # Update net\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    losseval += L.item()\n",
    "    if global_counter % eval_periodicity == 0:\n",
    "        ## evaluate stuff\n",
    "        model.eval()\n",
    "        metrics_dict = eval_model_on_DF(model, dev_dataframes_dict, batch_size=16, global_counter=global_counter)\n",
    "        print('eval metrics are:', metrics_dict)\n",
    "        writer.add_scalars('Metrics/dev', metrics_dict, global_counter)\n",
    "        \n",
    "    if global_counter % eval_periodicity == 0:\n",
    "        #print(f'Accuracies SST2: {metrics[0]} ---- QQP: {metrics[1]}')\n",
    "        print(f'################### GLOBAL COUNTER {global_counter} ###################')\n",
    "        print(f'Iterations per second: {eval_periodicity/(time.time()-t)}')\n",
    "        t = time.time()\n",
    "        print(f'Accuracies QQP: {acceval/eval_periodicity}')\n",
    "        print(f'Global Loss: {losseval/eval_periodicity}')\n",
    "        losseval, acceval = 0, 0\n",
    "    global_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "limit = 3\n",
    "indices = list(range(50))\n",
    "idx_combinations = [indices[s:e] for s, e in itertools.combinations(range(len(indices)+1), 2)]\n",
    "#print(idx_combinations)\n",
    "\n",
    "\n",
    "\n",
    "print(len(idx_combinations))\n",
    "print(idx_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, mean_shift\n",
    "\n",
    "DBSCAN_algorithm = DBSCAN(eps=0.7, min_samples=2)\n",
    "inp = np.random.rand(48, 768)\n",
    "clustered = DBSCAN_algorithm.fit_predict(inp)\n",
    "\n",
    "ms = mean_shift(inp)\n",
    "print(ms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "T = torch.tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "Y = T[1:1,:]\n",
    "U = torch.Tensor(1, 2)\n",
    "I = torch.cat([T, U], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([1, 2])\n",
    "U = torch.tensor([3, 4])\n",
    "H = torch.Tensor(1, 3)\n",
    "L = [U, H]\n",
    "I = torch.cat(L, dim=0)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-06 15:18:10.030791\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import datetime;\n",
    "ts = str(datetime.datetime.now())\n",
    "print(str(ts))\n",
    "print(type(str(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstpart/second'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join('firstpart', 'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 5, 4, 6, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "from sklearn import cluster\n",
    "T1 = torch.rand(3, 100)\n",
    "T1 = nn.normalize(T1, p=2, dim=-1)\n",
    "T2 = torch.rand(4, 100) - torch.rand(4, 100)\n",
    "T2 = nn.normalize(T2, p=2, dim=-1)\n",
    "T = torch.cat([T1, T2], dim=0)\n",
    "cl = cluster.MeanShift(bandwidth=0.5) # n_samples, n_features\n",
    "clustering = cl.fit(T)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_batch_SST2_from_indices at 0x11dfa1e18>\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "config = yaml.safe_load('./config/datasets.yml')\n",
    "with open('./config/datasets.yml', 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.Loader)\n",
    "#print(config)\n",
    "print(config['SST2']['get_batch_fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for dataset in config['datasets']:\n",
    "    dataframes[dataset] = {}\n",
    "    for kind in ['train', 'test', 'dev']:\n",
    "        dataframes[dataset][kind] = pd.read_csv(config[dataset]['path'][kind], sep='\\t')\n",
    "\n",
    "counter = {dataset: config[dataset]['counter'] for dataset in config['datasets']}\n",
    "batch_size = {dataset: config[dataset]['batch_size'] for dataset in config['datasets']}\n",
    "n_batches = {dataset: math.floor(len(dataframes[dataset]['train'])/batch_size[dataset]) \n",
    "             for dataset in config['datasets']}\n",
    "get_batch_function = {dataset: config[dataset]['get_batch_fn'] for dataset in config['datasets']}\n",
    "dev_dataframes_dict = {dataset: dataframe[dataset]['dev'] for dataset in config['datasets']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0      id    qid1    qid2  \\\n",
      "0              16      16      33      34   \n",
      "1              18      18      37      38   \n",
      "2              24      24      49      50   \n",
      "3              27      27      55      56   \n",
      "4              38      38      77      78   \n",
      "5              45      45      91      92   \n",
      "6              57      57     115     116   \n",
      "7              59      59     119     120   \n",
      "8              60      60     121     122   \n",
      "9              62      62     125     126   \n",
      "10             79      79     159     160   \n",
      "11             96      96     193     194   \n",
      "12            166     166     333     334   \n",
      "13            182     182     365     366   \n",
      "14            188     188     377     378   \n",
      "15            191     191     383     384   \n",
      "16            197     197     395     396   \n",
      "17            211     211     423     424   \n",
      "18            212     212     425     426   \n",
      "19            216     216     433     434   \n",
      "20            220     220     441     442   \n",
      "21            225     225     451     452   \n",
      "22            226     226     453     454   \n",
      "23            236     236     473     474   \n",
      "24            238     238     477     478   \n",
      "25            239     239     479     480   \n",
      "26            240     240     481     482   \n",
      "27            247     247     495     496   \n",
      "28            261     261     523     524   \n",
      "29            284     284     568     569   \n",
      "...           ...     ...     ...     ...   \n",
      "39092      404036  404036  537662  537663   \n",
      "39093      404042  404042   42529    4396   \n",
      "39094      404064  404064  537695  537696   \n",
      "39095      404066  404066  537698  537699   \n",
      "39096      404074  404074  537710  537711   \n",
      "39097      404083  404083  537718  253061   \n",
      "39098      404105  404105  537749  537750   \n",
      "39099      404108  404108  408060  494682   \n",
      "39100      404123  404123  537765  537766   \n",
      "39101      404129  404129  537772  537773   \n",
      "39102      404132  404132  217139  446491   \n",
      "39103      404143  404143  120688  336559   \n",
      "39104      404146  404146  243380  537796   \n",
      "39105      404149  404149  251294  537799   \n",
      "39106      404158  404158  154329  537807   \n",
      "39107      404159  404159  537808  537809   \n",
      "39108      404175  404175  537822  537823   \n",
      "39109      404176  404176  537824  537825   \n",
      "39110      404185  404185   19724  315898   \n",
      "39111      404200  404200    9026    9421   \n",
      "39112      404204  404204  537853  537854   \n",
      "39113      404210  404210  537860  537861   \n",
      "39114      404215  404215  537866  537867   \n",
      "39115      404216  404216   33176   43097   \n",
      "39116      404217  404217  537868   67173   \n",
      "39117      404227  404227  537875  537876   \n",
      "39118      404238  404238  311023  177595   \n",
      "39119      404243  404243   27696  146706   \n",
      "39120      404274  404274  178643   87385   \n",
      "39121      404279  404279  537920  537921   \n",
      "\n",
      "                                               question1  \\\n",
      "0                           What does manipulation mean?   \n",
      "1      Why are so many Quora users posting questions ...   \n",
      "2      What does it mean that every time I look at th...   \n",
      "3      Does society place too much importance on sports?   \n",
      "4                            How do we prepare for UPSC?   \n",
      "5      What is the quickest way to increase Instagram...   \n",
      "6              What are some good rap songs to dance to?   \n",
      "7                What are the best ways to learn French?   \n",
      "8      How do I download content from a kickass torre...   \n",
      "9      How is the new Harry Potter book 'Harry Potter...   \n",
      "10                              What is purpose of life?   \n",
      "11     What was it like to attend Caltech with Jeremy...   \n",
      "12     Are government employees eligible to Sukanya S...   \n",
      "13     What if I hired two private eyes and ordered t...   \n",
      "14     Is it possible to pursue many different things...   \n",
      "15                   How can I stop being so possessive?   \n",
      "16     What are some must watch TV shows before you die?   \n",
      "17     How headphones work as an Antenna to play FM r...   \n",
      "18     What are the best career growth technologies f...   \n",
      "19     I got a 3.8 GPA. Is it enough to get into top ...   \n",
      "20                             How do I earn from Quora?   \n",
      "21     How scary is it to drive on the road to Hana g...   \n",
      "22     How do I create a new shell in a new terminal ...   \n",
      "23            How do you potty train a 4 months Pitbull?   \n",
      "24     Which online test series is best for GATE 2017...   \n",
      "25     What is the best material for understanding al...   \n",
      "26     What is the temperament of a Bullmastiff/Husky...   \n",
      "27                            Why can flash run so fast?   \n",
      "28        How do I choose a journal to publish my paper?   \n",
      "29        How can I make money online with free of cost?   \n",
      "...                                                  ...   \n",
      "39092  How one can get into Indian foreign services a...   \n",
      "39093                          What do sperm taste like?   \n",
      "39094  Are cookies stored on my device even if I have...   \n",
      "39095                                     Top paid jobs?   \n",
      "39096                              Top mnc requitrments?   \n",
      "39097  How much margin can I get by taking amul franc...   \n",
      "39098  What are the best Berkeley classrooms/rooms/pl...   \n",
      "39099        What are the top things to do in Barcelona?   \n",
      "39100  Where is a reliable mobile repair shop to get ...   \n",
      "39101                   What defines an outgoing person?   \n",
      "39102  What are the major reasons for the prevalence ...   \n",
      "39103  What are some good UK universities in computer...   \n",
      "39104  How does Facebook choose who shows up in the c...   \n",
      "39105  1GB is equal to how many MB? Google is showing...   \n",
      "39106  Which mechanical project should I do if I am i...   \n",
      "39107                    How can I learn more about CRM?   \n",
      "39108                   How can I live alone in college?   \n",
      "39109          Are there any other Cleganes in Westeros?   \n",
      "39110  If someone blocks me on WhatsApp, how can I un...   \n",
      "39111  What are some CV worthy online Digital Marketi...   \n",
      "39112                     How do you dye red hair brown?   \n",
      "39113  What kind of user interests do inMobi and AdMo...   \n",
      "39114         What is it like to work with Aaron Sorkin?   \n",
      "39115       What are your new year resolutions for 2017?   \n",
      "39116  What more shocking moves can be expected by th...   \n",
      "39117  How does Russian politics affect Australia and...   \n",
      "39118  How do you know if a molecule has a single, do...   \n",
      "39119  How does Quora determine how many views an ans...   \n",
      "39120  Will the next generation of parenting change o...   \n",
      "39121  What is the minimum time required to build a f...   \n",
      "\n",
      "                                               question2  is_duplicate  \n",
      "0                          What does manipulation means?             1  \n",
      "1      Why do people ask Quora questions which can be...             1  \n",
      "2       How many times a day do a clock’s hands overlap?             0  \n",
      "3               How do sports contribute to the society?             0  \n",
      "4                    How do I prepare for civil service?             1  \n",
      "5      How can we increase our number of Instagram fo...             0  \n",
      "6                   What are some of the best rap songs?             0  \n",
      "7                         How do I learn french genders?             0  \n",
      "8                       Is Kickass Torrents trustworthy?             0  \n",
      "9                How bad is the new book by J.K Rowling?             1  \n",
      "10     What's the purpose of life? What is life actua...             1  \n",
      "11      Who are some notable folks who attended Caltech?             0  \n",
      "12                  Can a Government employee apply IEC?             0  \n",
      "13     Would I be able to hire two private investigat...             1  \n",
      "14     How do I refuse to chose between different thi...             1  \n",
      "15     How do I stop being over possessive for a pers...             1  \n",
      "16                    Are there any must watch TV shows?             1  \n",
      "17          Why don't phones have a built-in FM antenna?             0  \n",
      "18                Himalayan or Duke KTM 200 for touring?             0  \n",
      "19     Is a 3.8 GPA sufficient to get into a top school?             1  \n",
      "20                            Can I earn money on Quora?             1  \n",
      "21     Do I need a four-wheel-drive car to drive all ...             0  \n",
      "22     How do I create a new Terminal and new shell i...             1  \n",
      "23     How do I potty train my two-month-old Labrador...             1  \n",
      "24     Which test series is the best for GATE compute...             1  \n",
      "25     What math does a complete newbie need to under...             0  \n",
      "26         What is the temperament of a Corgi-Husky mix?             0  \n",
      "27     The Flash (DC character): How fast can the Fla...             0  \n",
      "28                          Where do I publish my paper?             1  \n",
      "29                        How do I to make money online?             1  \n",
      "...                                                  ...           ...  \n",
      "39092  Can an Indian Foreign Service officer get a fo...             0  \n",
      "39093                          What does cum taste like?             1  \n",
      "39094  I have 10 years of experience in human resourc...             0  \n",
      "39095  I want to make a center based hotel, what cent...             0  \n",
      "39096   How do I deal with a neighbor who is too clingy?             0  \n",
      "39097  How automata theory applicable in text process...             0  \n",
      "39098  What are the best Stanford classrooms/rooms to...             0  \n",
      "39099       What are the best things to do in Barcelona?             1  \n",
      "39100  Where is a reliable mobile repair shop to get ...             0  \n",
      "39101                        How can I get the outgoing?             0  \n",
      "39102  What are the main reasons for the prevalance o...             1  \n",
      "39103  What university would you recommend in the UK ...             1  \n",
      "39104  How are the list of friends' faces displayed i...             0  \n",
      "39105  How many kilobytes (kB) are there in a megabyt...             0  \n",
      "39106  Which mechanical engineering project should I ...             0  \n",
      "39107                    How can we learn more about IT?             0  \n",
      "39108              Should I live alone while in college?             0  \n",
      "39109         Are there any other Cleganes in the world?             1  \n",
      "39110  If someone blocks me in Hike will my messages ...             0  \n",
      "39111               Digital Marketing colleges in India?             1  \n",
      "39112    How do you get red hair dye out of white tiles?             0  \n",
      "39113      What kind of data Facebook has on it's users?             0  \n",
      "39114          What's it like to work with Aaron Sorkin?             1  \n",
      "39115  What will be your new year resolution for 2017...             1  \n",
      "39116  After demonetization, which could be next step...             0  \n",
      "39117  How do Russian politics and geostrategy affect...             1  \n",
      "39118  Is an atom a solid , liquid or gas ? Or is it ...             0  \n",
      "39119      How are the number of views on Quora counted?             1  \n",
      "39120  What kind of parents will the next generation ...             1  \n",
      "39121  What is a cheaper and quicker way to build an ...             0  \n",
      "\n",
      "[39122 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(counter)\n",
    "#print(batch_size)\n",
    "#print(n_batches)\n",
    "#print(get_batch_function)\n",
    "print(dataframes['QQP']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                      4\n",
      "id                                                              4\n",
      "qid1                                                            9\n",
      "qid2                                                           10\n",
      "question1       Which one dissolve in water quikly sugar, salt...\n",
      "question2                 Which fish would survive in salt water?\n",
      "is_duplicate                                                    0\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "model.data_utils.get_batch_SST2_from_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zeta Alpha",
   "language": "python",
   "name": "za_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
