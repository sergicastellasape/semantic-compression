{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/sergicastellasape/miniconda3/envs/za_conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Generic\n",
    "import argparse\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Data & Math\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Custom imports\n",
    "from model.model import End2EndModel\n",
    "\n",
    "# Neural Network Builder functions\n",
    "from model.builder.transformer import make_transformer\n",
    "from model.builder.bracketing import make_bracketer\n",
    "from model.builder.multitask import make_multitask_net\n",
    "from model.builder.generators import make_generator\n",
    "\n",
    "from model.utils import (\n",
    "    eval_model_on_DF,\n",
    "    str2bool,\n",
    "    str2list,\n",
    "    txt2list,\n",
    "    abs_max_pooling,\n",
    "    mean_pooling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "# load config file from datasets\n",
    "with open(\"./config/datasets.yml\", \"r\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.Loader)\n",
    "with open(\"./config/model.yml\", \"r\") as file:\n",
    "    model_config = yaml.load(file, Loader=yaml.Loader)\n",
    "with open(\"./config/optimizer.yml\", \"r\") as file:\n",
    "    optimizer_config = yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "#############################################################################\n",
    "############################### LOAD MODELS #################################\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "\n",
    "args = type('', (), {})()\n",
    "args.trf_out_layer = 10\n",
    "args.log_dir = \"./tensorboard\"\n",
    "args.checkpoint_id = None\n",
    "args.modules_to_load = ['multitasknet']\n",
    "args.modules_to_save = ['multitasknet']\n",
    "args.modules_to_train = ['multitasknet']\n",
    "args.train_comp = False\n",
    "args.eval_comp = False\n",
    "args.chunker = \"agglomerative\"  # [\"NNSimilarity\", \"agglomerative\", \"hard\", \"fixed\", \"freq\"]\n",
    "args.span = 11\n",
    "args.out_num = 1\n",
    "args.log_threshold = -52\n",
    "args.sim_threshold = 0\n",
    "args.dist_threshold = 1.71\n",
    "args.max_skip = 1\n",
    "args.pooling = \"mean_pooling\"  # [\"abs_max_pooling\", \"mean_pooling\", \"freq_pooling\", \"conv_att\"]\n",
    "\n",
    "transformer_net = make_transformer(args, device=device)\n",
    "bracketing = make_bracketer(args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: It was easy to spot her. All you needed to do was look at her socks. They were never a matching pair. One would be green while the other would be blue. One would reach her knee while the other barely touched her ankle. Every other part of her was perfect, but never the socks. They were her micro act of rebellion.\n",
      "\n",
      "Compression of 0.08571428571428572\n",
      "['it', 'was', 'easy', 'to', 'spot', 'her', '.', 'all', 'you', 'needed', 'to', 'do', 'was', 'look', 'at', 'her', 'socks', '.', 'they', 'were', 'never', 'a', 'matching', 'pair', '.', 'one', 'would', 'be', 'green', 'while', 'the', 'other', 'would', 'be', 'blue', '.', 'one', 'would', 'reach', 'her', 'knee', 'while', 'the', 'other', 'barely', 'touched', 'her', 'ankle', '.', 'every', 'other', 'part', 'of', 'her', 'was', 'perfect', ',', 'but', 'never', 'the', 'socks', '.', 'they', 'were', 'her', 'micro', 'act', 'of', 'rebellion', '.']\n",
      "['[CLS]', 'it was easy to spot her .', 'all you needed to do was', 'look at her socks . they were never a matching pair . one would be green', 'while the other would be blue . one would reach her knee while the other barely touched her ankle', '. every other part of her was perfect , but never the socks .', 'they were her micro act of rebellion .', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#sequence = \"the film would work much better as a video installation in a museum ,\\\n",
    "#            where viewers would be free to leave\"\n",
    "#sequence = \"On 25 February Cyborgo was stepped up in class for the Grade 2 Rendlesham Hurdle over three miles on heavy at Kempton Park Racecourse and started 8/11 favourite in a five-runner field.\"\n",
    "#sequence = \"On 25 February Cyborgo was stepped up in class for the Grade 2 Rendlesham Hurdle over three miles on heavy at Kempton Park Racecourse and started.\"\n",
    "sequence = \"It was easy to spot her. All you needed to do was look at her socks. They were never a matching pair. One would be green while the other would be blue. One would reach her knee while the other barely touched her ankle. Every other part of her was perfect, but never the socks. They were her micro act of rebellion.\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(sequence, \n",
    "                               add_special_tokens=True,\n",
    "                               return_special_tokens_mask=True,\n",
    "                               return_token_type_ids=True)\n",
    "s = sum(tokens['special_tokens_mask'])\n",
    "n_reg_tokens = len(tokens['input_ids']) - s\n",
    "\n",
    "tensor, mask, token_ids = transformer_net([sequence], return_extras=True)\n",
    "indices = bracketing.forward(tensor, masks_dict=mask, token_ids=token_ids)\n",
    "\n",
    "\n",
    "print(f\"Original sentence: {sequence}\")\n",
    "print(f\"\\nCompression of {(len(indices[0]) - 2) / n_reg_tokens}\")\n",
    "\n",
    "string_list = []\n",
    "for idx in indices[0]:\n",
    "    token_ids = [tokens['input_ids'][i] for i in idx]\n",
    "    string_list.append(tokenizer.decode(token_ids,\n",
    "                              skip_special_tokens=False, \n",
    "                              clean_up_tokenization_spaces=False))\n",
    "print(tokenizer.tokenize(sequence))\n",
    "print(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_colors(chunks, colors=['olive', 'purple', 'teal', 'grey']):\n",
    "    out = \"\"\n",
    "    i = 0\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.replace(\"#\", \"\\#\")\n",
    "        out += f\" \\\\textcolor[{colors[i]}][{chunk}]\".replace('[', '{').replace(']', '}')\n",
    "        i = i + 1 if i < 3 else 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\textcolor{olive}{it was easy to spot her .} \\textcolor{purple}{all you needed to do was} \\textcolor{teal}{look at her socks . they were never a matching pair . one would be green} \\textcolor{grey}{while the other would be blue . one would reach her knee while the other barely touched her ankle} \\textcolor{olive}{. every other part of her was perfect , but never the socks .} \\textcolor{purple}{they were her micro act of rebellion .}\n"
     ]
    }
   ],
   "source": [
    "out = write_colors(string_list[1:-1])\n",
    "#out = write_colors(tokenizer.tokenize(sequence))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\colors{stuff}\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a string ##stuff\n"
     ]
    }
   ],
   "source": [
    "s = \"##stuff\" \n",
    "print(f\"this is a string {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbbbbtbbbbttbbbb\n"
     ]
    }
   ],
   "source": [
    "s = 'patatta'\n",
    "s = s.replace('a', 'bbbb')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zeta Alpha",
   "language": "python",
   "name": "za_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
